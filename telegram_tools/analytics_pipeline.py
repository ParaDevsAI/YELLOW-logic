"""
analytics_pipeline.py - Script Combinado de An√°lise Inteligente

Este script combina 4 opera√ß√µes importantes em um pipeline √∫nico e eficiente:
1. metrics_snapshot.py - Atualiza√ß√£o de m√©tricas de tweets
2. cross_engagement_tracker.py - Rastreamento de engajamentos cruzados  
3. thread_identifier.py - Identifica√ß√£o de threads
4. generate_leaderboard.py - Gera√ß√£o de leaderboard

L√ìGICA INTELIGENTE:
- Executa APENAS 1x por dia (verifica se j√° rodou hoje)
- Processa tweets de forma otimizada por prioridade/idade
- Usa processamento paralelo quando poss√≠vel
- Logs detalhados e estat√≠sticas completas

FREQU√äNCIA RECOMENDADA: Di√°rio (00:00 UTC)
"""

import asyncio
import os
import httpx
import logging
import time
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv
from typing import Dict, List, Set, Tuple, Optional

# Imports do projeto
from bot.author_manager import get_supabase_client

# Configura√ß√£o de logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# Constantes
BATCH_SIZE = 100
API_DELAY = 1.5  # Delay entre chamadas para ser respeitoso com a API
TWEETS_PRIORITY_DAYS = 3  # Tweets com menos de 3 dias t√™m prioridade

class AnalyticsPipeline:
    """Classe principal que orquestra todo o pipeline de an√°lise."""
    
    def __init__(self):
        self.api_key = None
        self.supabase = None
        self.stats = {
            'tweets_processed': 0,
            'metrics_updated': 0,
            'threads_identified': 0,
            'engagements_found': 0,
            'leaderboard_generated': False,
            'start_time': None,
            'errors': 0
        }
        
    async def initialize(self):
        """Inicializa conex√µes e verifica configura√ß√£o."""
        logger.info("üöÄ --- Iniciando Pipeline de An√°lise Inteligente ---")
        
        load_dotenv()
        self.api_key = os.getenv("TWITTER_API_KEY")
        if not self.api_key:
            logger.critical("‚ùå TWITTER_API_KEY n√£o encontrada. Abortando.")
            return False
            
        self.supabase = await get_supabase_client()
        if not self.supabase:
            logger.critical("‚ùå Falha ao conectar com Supabase. Abortando.")
            return False
            
        self.stats['start_time'] = datetime.now(timezone.utc)
        logger.info("‚úÖ Inicializa√ß√£o conclu√≠da com sucesso.")
        return True
        
    async def check_if_already_ran_today(self) -> bool:
        """Verifica se o pipeline j√° foi executado hoje."""
        today_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        
        try:
            # Verifica na tabela leaderboard_history se j√° temos dados de hoje
            response = await asyncio.to_thread(
                self.supabase.table('leaderboard_history')
                .select('id')
                .gte('snapshot_timestamp', f'{today_date} 00:00:00+00')
                .lt('snapshot_timestamp', f'{today_date} 23:59:59+00')
                .limit(1)
                .execute
            )
            
            if response.data:
                logger.info(f"‚úÖ Pipeline j√° executado hoje ({today_date}). √öltima execu√ß√£o encontrada.")
                return True
                
            logger.info(f"üÜï Pipeline n√£o executado hoje ({today_date}). Execu√ß√£o necess√°ria.")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar execu√ß√£o di√°ria: {e}")
            return False
    
    async def get_tweets_to_analyze(self) -> List[Dict]:
        """
        Busca tweets que precisam de an√°lise, priorizando os mais recentes.
        """
        logger.info("üìä Buscando tweets para an√°lise...")
        
        try:
            # Busca tweets dos √∫ltimos 7 dias para an√°lise completa
            seven_days_ago = datetime.utcnow() - timedelta(days=7)
            
            response = await asyncio.to_thread(
                self.supabase.table('tweets')
                .select('tweet_id, author_id, createdat, is_thread_checked, views, likes, retweets')
                .gte('createdat', seven_days_ago.isoformat())
                .order('createdat', desc=True)
                .execute
            )
            
            if response.data:
                logger.info(f"üìà Encontrados {len(response.data)} tweets para an√°lise.")
                return response.data
            else:
                logger.warning("‚ö†Ô∏è Nenhum tweet encontrado para an√°lise.")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar tweets: {e}")
            return []
    
    async def update_tweet_metrics(self, tweets_batch: List[str]) -> int:
        """
        Atualiza m√©tricas de um lote de tweets usando a API.
        Baseado em metrics_snapshot.py
        """
        if not tweets_batch:
            return 0
            
        url = "https://api.twitterapi.io/twitter/tweets"
        headers = {"X-API-Key": self.api_key}
        params = {"tweet_ids": ",".join(tweets_batch)}
        
        try:
            async with httpx.AsyncClient(timeout=40.0) as client:
                response = await client.get(url, headers=headers, params=params)
                
                if response.status_code != 200:
                    logger.error(f"‚ùå Erro na API de m√©tricas: {response.status_code}")
                    return 0
                    
                data = response.json()
                tweets_data = data.get('data', [])
                
                if not tweets_data:
                    return 0
                
                # Prepara dados para atualiza√ß√£o
                history_records = []
                tweet_update_records = []
                snapshot_time = datetime.utcnow().isoformat()
                
                for tweet in tweets_data:
                    tweet_id = tweet.get('id')
                    author_id = tweet.get('author', {}).get('id')
                    
                    if not tweet_id or not author_id:
                        continue
                    
                    # Record para hist√≥rico
                    history_records.append({
                        'tweet_id': tweet_id,
                        'snapshot_at': snapshot_time,
                        'views': tweet.get('viewCount', 0),
                        'likes': tweet.get('likeCount', 0),
                        'retweets': tweet.get('retweetCount', 0),
                        'replies': tweet.get('replyCount', 0),
                        'quotes': tweet.get('quoteCount', 0),
                        'bookmarks': tweet.get('bookmarkCount', 0)
                    })
                    
                    # Record para atualiza√ß√£o principal
                    tweet_update_records.append({
                        'tweet_id': tweet_id,
                        'author_id': author_id,
                        'views': tweet.get('viewCount', 0),
                        'likes': tweet.get('likeCount', 0),
                        'retweets': tweet.get('retweetCount', 0),
                        'replies': tweet.get('replyCount', 0),
                        'quotes': tweet.get('quoteCount', 0),
                        'bookmarks': tweet.get('bookmarkCount', 0),
                    })
                
                # Salva em lote
                if history_records:
                    await asyncio.to_thread(
                        self.supabase.table('tweet_metrics_history').insert(history_records).execute
                    )
                    
                if tweet_update_records:
                    await asyncio.to_thread(
                        self.supabase.table('tweets').upsert(tweet_update_records, on_conflict='tweet_id').execute
                    )
                
                logger.info(f"‚úÖ M√©tricas atualizadas para {len(tweets_data)} tweets.")
                return len(tweets_data)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao atualizar m√©tricas: {e}")
            self.stats['errors'] += 1
            return 0
    
    async def check_thread_status(self, tweet_id: str) -> Optional[bool]:
        """
        Verifica se um tweet √© uma thread usando a API.
        Baseado em thread_identifier.py
        """
        url = "https://api.twitterapi.io/twitter/tweet/thread_context"
        headers = {"X-API-Key": self.api_key}
        params = {"tweetId": tweet_id}
        
        try:
            async with httpx.AsyncClient(timeout=40.0) as client:
                response = await client.get(url, headers=headers, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    thread_tweets = data.get('tweets', [])
                    is_thread = len(thread_tweets) >= 3
                    
                    # Atualiza no banco
                    await asyncio.to_thread(
                        self.supabase.table('tweets').update({
                            'is_thread': is_thread,
                            'is_thread_checked': True
                        }).eq('tweet_id', tweet_id).execute
                    )
                    
                    if is_thread:
                        self.stats['threads_identified'] += 1
                        
                    return is_thread
                else:
                    logger.warning(f"‚ö†Ô∏è Erro na API de thread para {tweet_id}: {response.status_code}")
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar thread {tweet_id}: {e}")
            self.stats['errors'] += 1
            return None
    
    async def analyze_cross_engagements(self, tweets_to_check: List[Dict]) -> int:
        """
        Analisa engajamentos cruzados entre embaixadores.
        Baseado em cross_engagement_tracker.py
        """
        logger.info("üîó Analisando engajamentos cruzados...")
        
        # Busca todos os IDs de embaixadores
        try:
            response = await asyncio.to_thread(
                self.supabase.table('authors').select('twitter_id').execute
            )
            ambassador_ids = {item['twitter_id'] for item in response.data} if response.data else set()
            
            if not ambassador_ids:
                logger.warning("‚ö†Ô∏è Nenhum embaixador encontrado.")
                return 0
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar embaixadores: {e}")
            return 0
        
        engagements_found = 0
        
        async with httpx.AsyncClient(timeout=40.0) as client:
            for tweet in tweets_to_check[:50]:  # Limita para n√£o sobrecarregar
                tweet_id = tweet['tweet_id']
                tweet_author_id = tweet['author_id']
                
                try:
                    # Busca retweets e replies em paralelo
                    retweets_task = self.fetch_retweeters(client, tweet_id)
                    replies_task = self.fetch_replies_and_quotes(client, tweet_id)
                    
                    results = await asyncio.gather(retweets_task, replies_task, return_exceptions=True)
                    retweeters = results[0] if not isinstance(results[0], Exception) else []
                    replies = results[1] if not isinstance(results[1], Exception) else []
                    
                    # Processa engajamentos
                    unique_engagements = set()
                    
                    # Processa retweets
                    for user in retweeters:
                        user_id = str(user.get('id'))
                        if user_id in ambassador_ids and user_id != tweet_author_id:
                            unique_engagements.add((tweet_id, tweet_author_id, user_id, 'retweet_or_quote', 2))
                    
                    # Processa replies
                    for reply_tweet in replies:
                        reply_author_id = str(reply_tweet.get('author', {}).get('id', ''))
                        if reply_author_id in ambassador_ids and reply_author_id != tweet_author_id:
                            unique_engagements.add((tweet_id, tweet_author_id, reply_author_id, 'reply', 2))
                    
                    # Salva engajamentos
                    if unique_engagements:
                        engagement_records = []
                        for eng in unique_engagements:
                            engagement_records.append({
                                'tweet_id': eng[0],
                                'tweet_author_id': eng[1],
                                'interacting_user_id': eng[2],
                                'action_type': eng[3],
                                'points_awarded': eng[4],
                                'created_at': tweet['createdat']
                            })
                        
                        try:
                            await asyncio.to_thread(
                                self.supabase.table('ambassador_engagements').upsert(
                                    engagement_records,
                                    on_conflict='tweet_id,interacting_user_id,action_type'
                                ).execute
                            )
                            engagements_found += len(engagement_records)
                        except Exception as e:
                            logger.error(f"‚ùå Erro ao salvar engajamentos: {e}")
                    
                    await asyncio.sleep(API_DELAY)  # Rate limiting respeitoso
                    
                except Exception as e:
                    logger.error(f"‚ùå Erro ao processar engajamentos do tweet {tweet_id}: {e}")
                    continue
        
        logger.info(f"‚úÖ Encontrados {engagements_found} novos engajamentos.")
        self.stats['engagements_found'] = engagements_found
        return engagements_found
    
    async def fetch_retweeters(self, client: httpx.AsyncClient, tweet_id: str) -> List[Dict]:
        """Busca usu√°rios que retweetaram."""
        url = "https://api.twitterapi.io/twitter/tweet/retweeters"
        headers = {"X-API-Key": self.api_key}
        params = {"tweetId": tweet_id}
        
        try:
            response = await client.get(url, headers=headers, params=params)
            if response.status_code == 200:
                return response.json().get('users', [])
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar retweeters: {e}")
        return []
    
    async def fetch_replies_and_quotes(self, client: httpx.AsyncClient, tweet_id: str) -> List[Dict]:
        """Busca replies e quotes."""
        url = "https://api.twitterapi.io/twitter/tweet/advanced_search"
        headers = {"X-API-Key": self.api_key}
        query = f"(conversation_id:{tweet_id}) OR (quoted_tweet_id:{tweet_id})"
        params = {"query": query, "queryType": "Latest"}
        
        try:
            response = await client.get(url, headers=headers, params=params)
            if response.status_code == 200:
                return response.json().get('tweets', [])
        except Exception as e:
            logger.error(f"‚ùå Erro ao buscar replies/quotes: {e}")
        return []
    
    async def generate_leaderboard(self):
        """
        Gera o leaderboard usando as fun√ß√µes RPC corretas do schema.
        Baseado em generate_leaderboard.py
        """
        try:
            # Executa a gera√ß√£o do leaderboard
            logger.info("üîÑ Gerando leaderboard...")
            
            # Usa a fun√ß√£o RPC calculate_leaderboard() do schema
            logger.info("‚ö° Calculando dados do leaderboard...")
            leaderboard_data = await asyncio.to_thread(
                self.supabase.rpc('calculate_leaderboard').execute
            )
            
            if not leaderboard_data.data:
                logger.warning("‚ö†Ô∏è Nenhum dado retornado do calculate_leaderboard")
                return False
            
            # Limpa e atualiza a tabela leaderboard
            logger.info("üîÑ Atualizando tabela leaderboard...")
            
            # Limpa tabela atual
            await asyncio.to_thread(
                self.supabase.table('leaderboard').delete().neq('user_id', 0).execute
            )
            
            # Prepara dados para inser√ß√£o com rank calculado
            current_time = datetime.now(timezone.utc).isoformat()
            insert_data = []
            
            for i, record in enumerate(leaderboard_data.data):
                insert_data.append({
                    'user_id': record['telegram_id'],
                    'rank': i + 1,  # Rank baseado na ordem dos resultados
                    'last_updated': current_time,
                    'telegram_name': record['telegram_name'],
                    'twitter_username': record['twitter_username'],
                    'count_tweets_text_only': record['count_tweets_text_only'],
                    'count_tweets_image': record['count_tweets_image'],
                    'count_tweets_thread': record['count_tweets_thread'],
                    'count_tweets_video': record['count_tweets_video'],
                    'total_score_from_tweets': float(record['total_score_from_tweets']),
                    'count_retweets_made': record['count_retweets_made'],
                    'count_comments_made': record['count_comments_made'],
                    'total_score_from_engagements': float(record['total_score_from_engagements']),
                    'total_score_from_telegram': float(record['total_score_from_telegram']),
                    'count_partner_introduction': record['count_partner_introduction'],
                    'count_hosting_ama': record['count_hosting_ama'],
                    'count_recruitment_ambassador': record['count_recruitment_ambassador'],
                    'count_product_feedback': record['count_product_feedback'],
                    'count_recruitment_investor': record['count_recruitment_investor'],
                    'total_score_from_contributions': float(record['total_score_from_contributions']),
                    'grand_total_score': float(record['grand_total_score'])
                })
            
            # Insere os dados atualizados
            await asyncio.to_thread(
                self.supabase.table('leaderboard').insert(insert_data).execute
            )
            
            # Salva snapshot no hist√≥rico
            logger.info("üíæ Salvando snapshot no hist√≥rico...")
            history_data = []
            for record in insert_data:
                history_record = record.copy()
                history_record['snapshot_timestamp'] = current_time
                history_data.append(history_record)
            
            await asyncio.to_thread(
                self.supabase.table('leaderboard_history').insert(history_data).execute
            )
            
            logger.info(f"üéâ Leaderboard gerado com sucesso! {len(insert_data)} usu√°rios processados.")
            self.stats['leaderboard_generated'] = True
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar leaderboard: {e}")
            self.stats['errors'] += 1
            return False
    
    async def run_pipeline(self):
        """Executa o pipeline completo de an√°lise."""
        try:
            # 1. Verifica se j√° executou hoje
            if await self.check_if_already_ran_today():
                logger.info("üìä Pipeline j√° executado hoje. Finalizando.")
                return
            
            # 2. Busca tweets para analisar
            tweets_to_analyze = await self.get_tweets_to_analyze()
            if not tweets_to_analyze:
                logger.warning("‚ö†Ô∏è Nenhum tweet para analisar. Finalizando.")
                return
            
            logger.info(f"üìà Iniciando an√°lise de {len(tweets_to_analyze)} tweets...")
            
            # 3. Processa tweets em lotes
            tweet_ids = [tweet['tweet_id'] for tweet in tweets_to_analyze]
            
            # 3a. Atualiza m√©tricas em lotes
            logger.info("üìä Atualizando m√©tricas dos tweets...")
            for i in range(0, len(tweet_ids), BATCH_SIZE):
                batch = tweet_ids[i:i + BATCH_SIZE]
                updated = await self.update_tweet_metrics(batch)
                self.stats['metrics_updated'] += updated
                await asyncio.sleep(API_DELAY)
            
            # 3b. Verifica threads para tweets n√£o verificados
            logger.info("üßµ Verificando status de threads...")
            tweets_to_check_threads = [t for t in tweets_to_analyze if not t.get('is_thread_checked', True)]
            
            for tweet in tweets_to_check_threads[:50]:  # Limita para n√£o sobrecarregar
                thread_result = await self.check_thread_status(tweet['tweet_id'])
                if thread_result is not None:
                    self.stats['tweets_processed'] += 1
                await asyncio.sleep(API_DELAY)
            
            # 3c. Analisa engajamentos cruzados
            await self.analyze_cross_engagements(tweets_to_analyze)
            
            # 4. Gera leaderboard se necess√°rio
            await self.generate_leaderboard()
            
            # 5. Relat√≥rio final
            await self.print_final_report()
            
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no pipeline: {e}")
            self.stats['errors'] += 1
    
    async def print_final_report(self):
        """Imprime relat√≥rio final das estat√≠sticas."""
        end_time = datetime.now(timezone.utc)
        duration = end_time - self.stats['start_time']
        
        logger.info("="*60)
        logger.info("üéØ --- RELAT√ìRIO FINAL DO PIPELINE DE AN√ÅLISE ---")
        logger.info("="*60)
        logger.info(f"‚è±Ô∏è  Dura√ß√£o total: {duration}")
        logger.info(f"üìä Tweets processados: {self.stats['tweets_processed']}")
        logger.info(f"üìà M√©tricas atualizadas: {self.stats['metrics_updated']}")
        logger.info(f"üßµ Threads identificadas: {self.stats['threads_identified']}")
        logger.info(f"üîó Engajamentos encontrados: {self.stats['engagements_found']}")
        logger.info(f"üèÜ Leaderboard gerado: {'‚úÖ Sim' if self.stats['leaderboard_generated'] else '‚ùå N√£o'}")
        logger.info(f"‚ùå Erros encontrados: {self.stats['errors']}")
        logger.info("="*60)
        logger.info("‚úÖ Pipeline de an√°lise conclu√≠do com sucesso!")


async def main():
    """Fun√ß√£o principal do pipeline."""
    pipeline = AnalyticsPipeline()
    
    if await pipeline.initialize():
        await pipeline.run_pipeline()
    else:
        logger.error("‚ùå Falha na inicializa√ß√£o. Pipeline abortado.")


if __name__ == "__main__":
    asyncio.run(main()) 